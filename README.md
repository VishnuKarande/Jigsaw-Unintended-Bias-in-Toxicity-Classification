# Jigsaw-Unintended-Bias-in-Toxicity-Classification
The Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.

Last year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.

# Data
The data for this project was taken from Kaggle
https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data

# To understand more on bias
https://www.youtube.com/watch?v=59bMh59JQDo

## project overview

### EDA
https://github.com/VishnuKarande/Jigsaw-Unintended-Bias-in-Toxicity-Classification/blob/main/Jigsaw_Unintended_Bias_in_Toxicity_Classification(EDA).ipynb
### text_preprocessing
https://github.com/VishnuKarande/Jigsaw-Unintended-Bias-in-Toxicity-Classification/blob/main/Jigsaw_Unintended_Bias_in_Toxicity_Classificationi_(text_preprocessing)_pynb(2).ipynb

## Future Work
The system is not able to handle High Volume of Data. #colab RAM crash problem is occured.

Down the line, I would like to transform this project from being machine learning based to deep learning based. Instead of using TF-IDF for preprocessing, I would like to explore word embedding techniques with tools like Word2Vec or more. I also think it would be useful to use a LSTM And GRU.

